{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - 4-5 junio 2018 - JMA\n",
    "\n",
    "*Contiene los Capítulos 4 y 5, que tratan Ortogonalidad y Determinantes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 4: Ortogonalidad\n",
    "Recordemos que, para vectores $\\mathbf{v},\\mathbf{w}\\in\\mathbb{R}^n$, hemos definido el *producto escalar* mediante la fórmula:\n",
    "$$\n",
    "\\mathbf{v}\\cdot\\mathbf{w} := \\sum_{i=1}^n v_iw_i,\\qquad \\Big(\\mathrm{y}\\,\\,\\mathrm{también:}\\quad \\mathbf{v}\\cdot\\mathbf{w} := \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|_{_\\,}\\,\\mathrm{cos }\\, \\theta\\, \\Big)\n",
    "$$\n",
    "Si los miramos como matrices $n\\times 1$ (o sea, como columnas), también podemos expresar el producto escalar como producto de matrices: $\\mathbf{v}\\cdot\\mathbf{w}=\\mathbf{v}^T\\mathbf{w}$. Notemos que:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\|\\mathbf{v} + \\mathbf{w}\\|^2 & = (\\mathbf{v} + \\mathbf{w})\\cdot (\\mathbf{v} + \\mathbf{w}) \\\\\n",
    "& = (\\mathbf{v}\\cdot\\mathbf{v})^2 + 2\\, \\mathbf{v}\\cdot\\mathbf{w} + (\\mathbf{w}\\cdot\\mathbf{w})^2 \\\\\n",
    "& = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2, \\quad (\\mathrm{usando}\\,\\, \\mathbf{v}\\cdot\\mathbf{w}=0)\\\\\n",
    "& = \\|\\mathbf{v} - \\mathbf{w}\\|^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Decimos que $\\mathbf{v},\\mathbf{w}$ son *ortogonales* (denotado $\\mathbf{v}\\bot\\mathbf{w}$) sii $\\mathbf{v}\\cdot\\mathbf{w}=0$, y que son *ortonormales* si, además de ortogonales, tienen longitud 1, en fórmulas: $\\mathbf{v}\\cdot\\mathbf{w}=0$ y, además, $\\mathbf{v}\\cdot\\mathbf{v}=1 =\\mathbf{w}\\cdot\\mathbf{w}$.\n",
    "\n",
    "Decimos que dos subespacios $V,W$ de $\\mathbb{R}^n$ son *ortogonales* sii para todo  $\\mathbf{v}\\in V$, y todo $\\mathbf{w}\\in W$, tenemos $\\mathbf{v}\\bot\\mathbf{w}$. \n",
    "### Sección 4.1 Ortogonalidad de los cuatro subespacios. \n",
    "<img src= \"imgLA/Fig_194_1.png\", width=800>\n",
    "\n",
    "En relación al punto 5, recordemos las siguientes afirmaciones (de la 2° clase):\n",
    ">La función $f_{A}:\\mathbb{R}^3\\rightarrow \\mathbb{R}^3$ está dada por $f_{A}(x,y,z)=(x,0)$. Los cuatro subespacios son:\n",
    ">\n",
    "-  $N(A)$ = plano $y,z$ (en $\\mathbb{R}^3$) = ${\\color{red}{núcleo}}$ de $f_A$. **dim** $=m-r$.\n",
    "- $C(A^T)$ = eje de las $x$ (en $\\mathbb{R}^3$) = ${\\color{blue}{complemento}}$ ${\\color{blue}{ortogonal}}$ del núcleo de $f_A$. **dim** $=r$.\n",
    "- $C(A)$ = eje de las $x$ (en $\\mathbb{R}^2$) = ${\\color{red}{imagen}}$ de $f_A$ ($\\mathrm{im}\\, f_A$). **dim** $=r$.\n",
    "- $N(A^T)$ = eje de las $y$ (en $\\mathbb{R}^2$) = ${\\color{blue}{complemento}}$ ${\\color{blue}{ortogonal}}$ de $\\mathrm{im}\\, f_A$. **dim** $=n-r$.\n",
    "## ${\\color{red}{\\Rightarrow}}$ Nota:\n",
    "Las afirmaciones que siguen al $segundo$ signo \"igual\" de cada renglón arriba, son afirmaciones $generales$, válidas para toda $A,\\, f_A$. Los subespacios \"rojos\" (núcleo e imagen de $f_A$) quedan definidos directamente a partir de $f_A$: son conceptos básicos de $cualquier$ función. Luego se determinan los subespacios \"azules\" como complementos ortogonales de los anteriores. Sabiendo que la función está definida por una matriz $A$, podemos interpretar a estos subespacios \"azules\", respectivamente, como el espacio \"de filas\" y el espacio \"núcleo a izquierda\" de la matriz $A$. \n",
    "\n",
    "Como $\\mathbf{x} = \\mathbf{x}_{\\mathrm{fila} } + \\mathbf{x}_{\\mathrm{núcleo}}$, tenemos:\n",
    "\n",
    "$$\n",
    "f_A(\\mathbf{x}) = f_A(\\mathbf{x}_{\\mathrm{fila} } + \\mathbf{x}_{\\mathrm{núcleo}}) =  f_A( \\mathbf{x}_{\\mathrm{fila} }) + f_A(\\mathbf{x}_{\\mathrm{núcleo}})=f_A(\\mathbf{x}_{\\mathrm{fila} } )+\\mathbf{0},\\quad\\Big(\\mathrm{de}\\,\\,\\mathrm{donde}:\\,\\,\\mathrm{im}\\,f_A=\\mathrm{im}\\,\\big(f_A\\big|_{\\mathrm{filas}}\\big)\\Big).\n",
    "$$\n",
    "\n",
    "Más aún, la descomposición $\\mathbf{x} = \\mathbf{x}_{\\mathrm{fila} } + \\mathbf{x}_{\\mathrm{núcleo}}$ es ${\\color{red}{única}}$. En efecto, si $\\mathbf{x} = \\mathbf{x}_{\\mathrm{fila} } + \\mathbf{x}_{\\mathrm{núcleo}}=  \\mathbf{x'}_{\\mathrm{fila} } + \\mathbf{x'}_{\\mathrm{núcleo}}$, entonces $\\mathbf{x}_{\\mathrm{fila} } - \\mathbf{x'}_{\\mathrm{fila}}=  \\mathbf{x'}_{\\mathrm{núcleo} } - \\mathbf{x}_{\\mathrm{núcleo}}$. Por lo tanto, $\\big(\\mathbf{x}_{\\mathrm{fila} } - \\mathbf{x'}_{\\mathrm{fila}}\\big),  \\big(\\mathbf{x'}_{\\mathrm{núcleo} } - \\mathbf{x}_{\\mathrm{núcleo}}\\big)\\in N(A)\\cap C(A^T)=Z=\\{\\mathbf{0}\\} $ (${\\color{red}{¿razón?}}$). De aquí podemos deducir que $\\big(f_A\\big|_{\\mathrm{filas}}\\big)$ es *inyectiva*. En efecto, si $\\big(f_A\\big|_{\\mathrm{filas}}\\big) (\\mathbf{x}_{\\mathrm{fila}}) = \\big(f_A\\big|_{\\mathrm{filas}}\\big)(\\mathbf{x'}_{\\mathrm{fila}})$, entonces $\\big(\\mathbf{x}_{\\mathrm{fila} } - \\mathbf{x'}_{\\mathrm{fila}}\\big)\\in N(A)\\cap C(A^T)$, o sea, son iguales.\n",
    "\n",
    "Podemos entonces concluir que $\\big(f_A\\big|_{\\mathrm{filas}}\\big): C(A^T)\\rightarrow \\mathrm{im}\\, f_A$, establece una *biyección* entre los espacios $C(A^T),\\,\\mathrm{im}\\, f_A$. Como además la biyección preserva las respectivas estructuras de espacio vectorial, obtenemos que los dos espacios son, en tanto espacios vectoriales, indistinguibles. Esta situación se expresa formalmente diciendo que los espacios son *isomorfos*. \n",
    "\n",
    "O sea, podemos identificar $C(A^T)$, el *espacio de las filas* de $A$ (subespacio, en general, de $\\mathbb{R}^m$), con $\\mathrm{im}\\, f_A$, el *espacio de columnas* de $A$ (subespacio de $\\mathbb{R}^n$). Esto explica, en particular, que dichos espacios tengan la *misma dimensión* $=r$, el *rango* de $A$. \n",
    "\n",
    "Tenemos entonces la siguiente imagen *revisada* de los cuatro subespacios:\n",
    "<img src= \"imgLA/Fig_197_1.png\", width=800>\n",
    "\n",
    "Podemos precisar aún más la situación mediante la siguiente:\n",
    "\n",
    "DEFINICIÓN: Sea $W$ un subespacio de $V$. El *complemento ortogonal* $W^\\bot$ de $W$ (en $V$) es, por definición, el conjunto de **todos** los vectores de $V$ que son ortogonales a $W$. Tenemos la siguiente importante fórmula:\n",
    "\n",
    "$$\n",
    "\\mathrm{dim}\\, W + \\mathrm{dim}\\, W^\\bot = \\mathrm{dim}\\, V.\n",
    "$$\n",
    "\n",
    "Con esta última *revisión*, nos queda la siguiente imagen de los cuatro subespacios:\n",
    "<img src= \"imgLA/Fig_199_1.png\", width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 4.2 Proyección ortogonal. \n",
    "<img src= \"imgLA/Fig_206_1.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobre el punto 1 del libro\n",
    "Sea $\\ell$ la recta generada por $\\mathbf{0}\\neq\\mathbf{a}\\in \\mathbb{R}^3$, $\\ell:=\\{t\\,\\mathbf{a}\\,|\\, t\\in\\mathbb{R}\\}$, y $p:\\mathbb{R}^3\\rightarrow \\ell$ la *proyección ortogonal* que mapea cada vector del espacio, **perpendicularmente** sobre la recta. Esta función puede definirse así: buscamos expresar todo $\\mathbf{b}\\in\\mathbb{R}^3$ como suma:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\mathbf{x} + \\mathbf{y}\\qquad\\qquad (1)\n",
    "$$\n",
    "\n",
    "de modo tal que $\\mathbf{x}\\in \\ell$, y que $\\mathbf{y}\\bot \\ell$. Si lo conseguimos, diremos que $\\mathbf{x}$ es la *proyección ortogonal* de $\\mathbf{b}$ en $\\ell$, denotado $p(\\mathbf{b}):=\\mathbf{x}$, y que $\\mathbf{y}= \\mathbf{b}-\\mathbf{x}$ es el término de *error*. La ecuación (1) nos quedará:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = p(\\mathbf{b}) + \\mathbf{error}\\, ,\\qquad \\mathrm{donde\\,\\,}p \\mathrm{\\,\\, es \\,\\, la\\,\\,} proyección \\,\\, ortogonal,\\,\\,\\mathrm{ y\\,\\,} \\big(\\mathbf{error}\\, \\bot\\, \\ell\\big)\\qquad (1')\n",
    "$$\n",
    "\n",
    "Calculamos. En primer lugar,  $\\mathbf{x}=t\\,\\mathbf{a}$ para algún $t\\in\\mathbb{R}$. Por (1), la condición de perpendicularidad dice que $0=\\mathbf{a}\\cdot (\\mathbf{b}-\\mathbf{x})= \\mathbf{a}\\cdot \\mathbf{b}-\\mathbf{a} \\cdot t\\,\\mathbf{a} = \\mathbf{a}\\cdot \\mathbf{b}-t\\,\\mathbf{a} \\cdot \\mathbf{a} $. O sea, \n",
    "\n",
    "$$\n",
    "t=\\frac{\\mathbf{a}\\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}} = \\frac{\\mathbf{a}\\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|^2}\\, ,\n",
    "\\qquad \\mathrm{de \\, donde}\\quad \\mathbf{x}=\\Big(\\frac{\\mathbf{a}\\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}}\\Big)\\, \\mathbf{a}. \\qquad (2)\n",
    "$$\n",
    "\n",
    "DEFINICIÓN: La proyección (sobre $\\ell$) está dada por la fórmula $p(\\mathbf{b}):= \\mathbf{x}= \\big(\\mathbf{a}\\cdot \\mathbf{b}\\, / \\, \\mathbf{a} \\cdot \\mathbf{a}\\big)\\, \\mathbf{a}$. \n",
    "\n",
    "NOTA. En (2) encontramos que $t$ es *único*. De ahí resulta que la descomposición (1), (1') es *única*. Notemos también que, con la notación del punto 2 del libro, el término de *error* $\\mathbf{e}=\\mathbf{b}- p(\\mathbf{b})$, es perpendicular a $\\ell$. La fórmula del libro expresa esta identidad usando multiplicación de matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobre el punto 3, 5 y 6 del libro (Proyección ortogonal sobre un subespacio)\n",
    "Este tema generaliza el punto 1: queremos proyectar sobre un subespacio $W$ de dimensión $m$, mientras que el punto 1 trata del caso especial en que el subespacio tiene dimensión 1.\n",
    "\n",
    "La primera observación es que usamos **exactamente** la *misma* definición de proyección $p:\\mathbb{R}^n \\rightarrow W$ que dimos en (1), con una modificación obvia: exigimos que $\\mathbf{x}$ pertenezca al $W$, en lugar de exigir pertenencia a $\\ell$. Como en (1), buscamos expresar todo $\\mathbf{b}\\in\\mathbb{R}^n$ como una suma  $\\mathbf{b}=\\mathbf{x} + \\mathbf{y}$ y, como en el caso de dimensión 1, definimos $p(\\mathbf{b}):= \\mathbf{x}=\\mathbf{b}-\\mathbf{y}$, y como antes, tenemos \n",
    "\n",
    "$$\n",
    "\\mathbf{b} = p(\\mathbf{b}) + \\mathbf{error},\\,  \\mathrm{donde\\,\\,}p \\mathrm{\\,\\, es \\,\\, la\\,\\,} proyección \\,\\, ortogonal\\,\\,\\mathrm{sobre\\,\\,} W,\\mathrm{\\,\\, y\\,\\,} \\big(\\mathbf{error}\\, \\bot\\, W\\big).\n",
    "$$\n",
    "\n",
    "Para obtener una fórmula similar a la encontrada en (2), supondremos que $W$ tiene por base el conjunto $B=\\{\\mathbf{a}_1,\\dots, \\mathbf{a}_m\\}$. Como exigimos $\\mathbf{x}\\in W$, debemos encontrar una combinación lineal $\\mathbf{x}=t_1\\,\\mathbf{a}_1 + \\cdots + t_m\\, \\mathbf{a}_m$. La otra condición es que $\\mathbf{y} \\bot W$, lo que equivale a exigir $\\mathbf{a}_i \\cdot \\mathbf{y}=0$, para $i=1,\\dots, m$. Notemos también que, como en el caso anterior, existe una matriz *proyección* $P$, de tamaño $n\\times n$, tal que $p(\\mathbf{b})=P\\, \\mathbf{b}$.\n",
    "\n",
    "Si formamos una matriz $A$ de tamaño $n\\times m$ cuyas columnas sean las componentes de cada $\\mathbf{a}_1,\\dots,\\mathbf{a}_m$, resulta que $\\mathbf{x}=A\\mathbf{t}$, donde las componentes de $\\mathbf{t}$ son $t_1,\\dots,t_m$. Es claro que $\\mathbf{y}\\bot W$ sii $A^T\\mathbf{y}=\\mathbf{0}$. Aplicado a $\\mathbf{y}=\\mathbf{b}-\\mathbf{x}$, tenemos:\n",
    "\n",
    "$$\n",
    "A^T\\mathbf{b}= A^TA \\mathbf{t}\\, , \\qquad \\mathrm{de\\, donde}\\quad \\mathbf{t}= (A^TA)^{-1}A^T \\mathbf{b}.\\, \\mathrm{Entonces}\\,\\, p(\\mathbf{b}):= \\mathbf{x}= A(A^TA)^{-1}A^T\\mathbf{b} = P\\mathbf{b}\n",
    "$$\n",
    "\n",
    "Vemos que $P= A(A^TA)^{-1}A^T$. Como toda proyección, $P^2=P$, ya que $P^2 = \\big(A(A^TA)^{-1}A^T\\big) \\, \\big(A(A^TA)^{-1}A^T\\big)=P$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sobre el punto 4 del libro: si $A$ tiene columnas independientes, entonces $A^TA$ es invertible (además de simétrica)\n",
    "\n",
    "Demostraremos que $A^T A\\mathbf{x}=\\mathbf{0}$ implica $A\\mathbf{x}=\\mathbf{0}$. Sea $\\mathbf{0}=A^T A\\mathbf{x}$, y multipliquemos a izquierda smbos miembros por $\\mathbf{x}^T$. El resultado: \n",
    "\n",
    "$$\n",
    "\\mathbf{0}=\\mathbf{x}^T A^T A\\mathbf{x}=A\\mathbf{x} \\cdot A\\mathbf{x}=\\|A\\mathbf{x}\\|^2,\n",
    "$$\n",
    "\n",
    "muestra que $A\\mathbf{x}=\\mathbf{0}$, pues su longitud es nula. Ahora bien, sabemos que las columnas de $A$ son independientes sii $A\\mathbf{x}=\\mathbf{0}$ implica $\\mathbf{x}=\\mathbf{0}$. Hemos demostrado que si las columnas de $A$ son independientes, entonces también lo serán las de $A^TA$. Pero como $A^TA$ es cuadrada, la independencia de sus columnas dice que $A^TA$ es invertible, como deseábamos probar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sección 4.3 Aproximación por cuadrados mínimos. \n",
    "<img src= \"imgLA/Fig_219_1.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos primero un ejemplo (pág. 219). Dados tres puntos $(0,6),(1,0)$ y $(2,0)$, queremos encontrar una recta que se ajuste \"lo mejor posible\" a los puntos dados. Planteamos una ecuación lineal $C+D\\, t=b$, y vemos qué condiciones impone la exigencia de que contenga los 3 puntos:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "t=0 &\\qquad C+D\\, 0=6\\\\\n",
    "t=1 &\\qquad C + D\\, 1=0\\\\\n",
    "t=2 &\\qquad C+D\\, 2=0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Obtenemos un sistema $A\\mathbf{x}=\\mathbf{b}$, de 3 ecuaciones con 2 incógnitas ($C,D$), que *no* tiene solución, donde:\n",
    "<img src= \"imgLA/Fig_220_1.png\", width=800>\n",
    "\n",
    "Esta ecuación nos permite dar una interpretación geométrica del problema. Que el sistema no tenga solución significa que $\\mathbf{b}$ no es una combinación lineal de $\\mathbf{a}_1,\\mathbf{a}_2$, las columnas de $A$. Es decir, que no pertenece a $C(A)$, el espacio generado por las columnas de $A$. Esto nos da una forma de resolver el problema y a la vez precisar el significado de \"lo mejor posible\": reemplazamos $\\mathbf{b}$ por el vector de $C(A)$ que está más cerca de $\\mathbf{b}$, es decir, por $p(\\mathbf{b})$, la proyección de $\\mathbf{b}$ sobre $C(A)$. \n",
    "\n",
    "O sea, reemplazamos el sistema *irresoluble* $A\\mathbf{x}=\\mathbf{b}$, por el sistema $A\\mathbf{x}=p(\\mathbf{b})$   (3), cuya solución nos da la recta con el \"mejor ajuste posible\" a los puntos. El **error** cometido es el mínimo posible, e igual a la longitud de $\\mathbf{b}-p(\\mathbf{b})$.\n",
    "\n",
    "NOTA: A partir del sistema (3): $A\\mathbf{x}=p(\\mathbf{b})=A(A^T A)^{-1}A^T\\mathbf{b}$, pasamos, multiplicando a izquierda por $A^T$ al sistema (4): $A^T A\\mathbf{x}=A^T\\mathbf{b}$. Recíprocamente, podemos pasar de (4) a (3) multiplicando a izquierda por $A(A^TA)^{-1}$. En CONCLUSIÓN, (3) y (4) tienen exactamente la misma solución, denotada $\\widehat{\\mathbf{x}}$. Además, es claro que el sistema (4) tiene solución única (${\\color{red}{\\mathrm{¿por\\,\\,qué}?}}$). En principio, es más sencillo resolver (4) porque (3) requiere calcular una inversa. Esto explica el punto 1 del libro (arriba).\n",
    "\n",
    "\n",
    "En el ejemplo de arriba, tenemos:\n",
    "\n",
    "$$\n",
    "A^TA=\n",
    " \\begin{pmatrix}\n",
    "  3 & 3 \\\\\n",
    "  3 & 5 \n",
    " \\end{pmatrix}\n",
    " \\qquad \n",
    " A^T \\mathbf{b} =\n",
    " \\begin{pmatrix}\n",
    "  6 \\\\\n",
    "  0\n",
    " \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "de donde obtenemos $\\widehat{\\mathbf{x}}=(5,-3)$, y $p(\\mathbf{b})=A\\widehat{\\mathbf{x}}=(5,2,-1)$. Como $p(\\mathbf{b})\\in C(A)$, el sistema $A\\mathbf{x}=p(\\mathbf{b})$ *tiene solución*. Los valores $(C,D)$ así obtenidos nos dan la recta que aproxima los puntos por cuadrados mínimos. Las componentes del vector **error** $\\mathbf{e}=(1,-2,1)$, coinciden con la diferencia entre las coordenadas-$y$ de los puntos y los correspondientes de la recta. Las componentes de la proyección $p(\\mathbf{b})=(5,2,-1)$, coinciden con las coordenas-$y$ *sobre la recta*, en los puntos en cuestión.\n",
    "<img src= \"imgLA/Fig_221_1.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\color{red} \\Rightarrow}$ OBSERVAR que la matriz $A$ tiene, respecto a la del libro, las columnas permutadas. ${\\color{red} \\Leftarrow}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([[0,1],[1,1],[2,1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6],\n",
       "        [0],\n",
       "        [0]]), array([[0],\n",
       "        [1],\n",
       "        [2]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.array([[6],[0],[0]])\n",
    "x=np.array([[0],[1],[2]])\n",
    "b, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.]), array([ 5.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, c = la.lstsq(A, b)[0]\n",
    "m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkzvx/lxF2Gofs2pYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6176e1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, b, 'o', label='Original data', markersize=15)\n",
    "plt.plot(x, m*x + c, 'r', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El caso general.\n",
    "Dados $m\\ge 3$ puntos que en los \"tiempos\" $t_1,\\dots,t_m$ tienen \"alturas\" $b_1,\\dots,b_m$. Suponemos que la mejor recta $C+Dt$ que aproxima, pasa a distancias \"verticales\" $e_1,\\dots,e_m$ de los puntos. Ninguna recta pasa por todos los puntos, y los cuadrados mínimos optimizan (minimizan) $E=e_1^2+\\cdots+e_m^2$. \n",
    "\n",
    "Una recta pasa por todos los $m$ puntos cuando el sistema $A\\mathbf{x}=\\mathbf{b}$ tiene solución. Las incógnitas $C,D$ determinan una recta, de modo que $A$ tiene $n=2$ columnas. Para ajustar la recta a los puntos, tratamos de resolver este sistema de $m$ ecuaciones con $n=2$ incógnitas. \n",
    "<img src= \"imgLA/Fig_223_1.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_223_2.png\", width=800>\n",
    "\n",
    "En general, las matrices del sistema $A^TA\\mathbf{x}=A^T\\mathbf{b}$ tienen la forma:\n",
    "\n",
    "<img src= \"imgLA/Fig_223_3.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_223_4.png\", width=800>\n",
    "\n",
    "En RESUMEN:\n",
    "\n",
    "<img src= \"imgLA/Fig_224_1.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_224_2.png\", width=800>\n",
    "\n",
    "FINALMENTE, aquí tenemos un gráfico mostrando \"the big picture\":\n",
    "\n",
    "<img src= \"imgLA/Fig_222_1.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 4.4 Bases ortonormales y proceso de ortonormalización de Gram-Schmidt\n",
    "<img src= \"imgLA/Fig_233_1.png\", width=800>\n",
    "\n",
    "La primera parte de esta sección está dedicada a mostrar que ${\\color{red}{\\mathrm{\"orthogonal\\,\\, IS\\,\\, GOOD!\"}}}$. Damos unos pocos ejemplos de la esta sección y le dejamos al estudiante leer el resto de la sección y mirar los ejemplos. La segunda parte de la sección trata del *proceso de ortonormalización de Gram-Schmidt*, que discutimos aquí con detalle.\n",
    "\n",
    "<img src= \"imgLA/Fig_234_1.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_235_1.png\", width=800>\n",
    "\n",
    "### El proceso de ortonormalización de Gram-Schmidt\n",
    "Suponemos dados tres vectores linealmente independientes $\\mathbf{a},\\mathbf{b},\\mathbf{c}$ en $\\mathbb{R}^3$. A partir de estos datos, el proceso de Gram-Schmidt produce 3 nuevos vectores del espacio, $\\mathbf{q}_1, \\mathbf{q}_1, \\mathbf{q}_3$, que son *ortonormales*. El proceso es inductivo, y funciona igualmente si reemplazamos $3$ por un $n$ arbitrario.\n",
    "\n",
    "-  Primer paso. Normalización de $\\mathbf{a}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{q}_1:= \\frac{\\mathbf{a}}{\\|\\mathbf{a}\\|}\\, ,\\qquad\\qquad \\mathrm{posible\\,\\, porque\\,\\,} \\|\\mathbf{a}\\|\\neq 0.\n",
    "$$\n",
    "-  Segundo paso. Se usan $\\mathbf{q}_1$ y $\\mathbf{b}$ para obtener un $\\mathbf{q}_2$ de longitud 1 y perpendicular a $\\mathbf{q}_1$. Para logarlo, sea $\\ell$ la recta generada por $\\mathbf{q_1}$ (un subespacio de dimensión 1), y $p$ la proyección ortogonal sobre $\\ell$. Entonces $\\big(\\mathbf{b}-p(\\mathbf{b})\\big)\\bot \\mathbf{q}_1$, y podemos definir $\\mathbf{q}_2$ normalizando este vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{q}_2:=\\frac{\\mathbf{b}-p(\\mathbf{b})}{\\|\\mathbf{b}-p(\\mathbf{b})\\|}\\, ,\\qquad\\qquad \\mathrm{posible\\,\\, porque\\,\\,} \\|\\mathbf{b}-p(\\mathbf{b})\\|\\neq 0.\n",
    "$$\n",
    "\n",
    "-  Tercer paso. Se usan $\\mathbf{q}_1,\\mathbf{q}_2$ y $\\mathbf{c}$ para obtener un $\\mathbf{q}_3$ de longitud 1 y perpendicular a $\\mathbf{q}_1,\\mathbf{q}_2$. Sea $W$ el plano (subespacio de dimensión 2) generado por $\\mathbf{q}_1, \\mathbf{q}_2$, y $p$ la proyección ortogonal sobre $W$. Nuevamente, $\\big(\\mathbf{c}-p(\\mathbf{c})\\big)\\bot W$, y podemos definir $\\mathbf{q}_3$ normalizando, porque el vector es no nulo.\n",
    "\n",
    "Esto puede traducirse, igual que en las secciones anteriores, a lenguage matricial, definiendo una matriz $A$ de tamaño $3\\times 3$, con columnas dadas por los vectores $\\mathbf{a},\\mathbf{b},\\mathbf{c}$.\n",
    "\n",
    "### Factorización $QR$.\n",
    "\n",
    "<img src= \"imgLA/Fig_239_1.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_239_2.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [-1,  0, -3],\n",
       "       [ 0, -2,  3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=np.array([[1,2,3],[-1,0,-3],[0,-2,3]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.40824829,  0.57735027],\n",
       "       [ 0.70710678, -0.40824829,  0.57735027],\n",
       "       [-0.        ,  0.81649658,  0.57735027]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, r = la.qr(B)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41421356, -1.41421356, -4.24264069],\n",
       "       [ 0.        , -2.44948974,  2.44948974],\n",
       "       [ 0.        ,  0.        ,  1.73205081]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 5 - Determinantes\n",
    "### Sección 5.1  - Determinantes de matrices $2\\times 2$. Propiedades\n",
    "\n",
    "En esta sección se define $\\mathrm{det\\,}A$ para matrices $2\\times 2$, y se dan ejemplos. También se demuestran, en este caso particular, las principales propiedades que definen al determinante, incluyendo la importante regla del producto: $\\mathrm{det\\,}A$\\mathrm{det\\,}A = En esta sección se define $\\mathrm{det\\,}A$ para matrices $2\\times 2$, y se dan ejemplos. También se demuestran, en este caso particular, las principales propiedades que definen al determinante, incluyendo la importante regla del producto: \n",
    "\n",
    "$$\n",
    "\\mathrm{det\\,} (AB)=(\\mathrm{det\\,}A)( \\mathrm{det\\,}B).\n",
    "$$\n",
    "\n",
    "<img src= \"imgLA/Fig_247_1.png\", width=800>\n",
    "\n",
    "Para calcular determinantes con Python, podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5,  1,  1],\n",
       "       [ 1, -5,  1],\n",
       "       [ 1,  1, -5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([[-5,1,1],[1,-5,1],[1,1,-5]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-108.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sla.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedades del determinante\n",
    "Considerada como una función de columnas (o de filas), det es la **única** función que tiene las siguientes 3 propiedades:\n",
    "\n",
    "<img src= \"imgLA/Fig_249_1.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_249_2.png\", width=800>\n",
    "\n",
    "A partir de ests propiedades, se pueden deducir todas las propiedades del determinante. Algunas de esas \"reglas\" explicitadas:\n",
    "\n",
    "<img src= \"imgLA/Fig_250_1.png\", width=800>\n",
    "\n",
    "\n",
    "<img src= \"imgLA/Fig_250_2.png\", width=800>\n",
    "\n",
    "Una *conclusión importante*: si obtenemos $U$ por eliminación a partir de $A$, entonces $\\mathrm{det\\,} U= \\mathrm{det\\, }A$.\n",
    "\n",
    "<img src= \"imgLA/Fig_250_3.png\", width=800>\n",
    "\n",
    "El determinante de matrices triangulares es fácil de calcular:\n",
    "<img src= \"imgLA/Fig_251_1.png\", width=800>\n",
    "\n",
    "El determinante caracteriza invertibilidad de matrices:\n",
    "<img src= \"imgLA/Fig_251_2.png\", width=800>\n",
    "\n",
    "\n",
    "La IMPORTANTE fórmula del producto, y su consecuencia para el deeterminante de la matriz inversa.\n",
    "\n",
    "<img src= \"imgLA/Fig_252_1.png\", width=800>\n",
    "\n",
    "Demostración de la fórmula del producto.\n",
    "<img src= \"imgLA/Fig_252_2.png\", width=800>\n",
    "\n",
    "El determinante de la transpuesta, con demostración.\n",
    "<img src= \"imgLA/Fig_252_3.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sección 5.2 - Permutaciones y cofactores\n",
    "\n",
    "<img src= \"imgLA/Fig_258_1.png\", width=800>\n",
    "\n",
    "Los cofactores $C_{ij}$ de una matriz $A$ de tamaño $n\\times n$, son $n^2$ números definidos así: primero consideramos submatrices $M_{ij}$ de tamaño $(n-1)\\times (n-1)$, obtenidas eligiendo la posición $ij$ y tachando esa fila y esa columna (o sea, la fila $i$ y la columna $j$). Los cofactores $C_{ij}$ se definen como el determinante de las submatrices, con un signo:\n",
    "\n",
    "$$\n",
    "C_{ij}:= (-1)^{i+j} \\mathrm{det\\,} M_{i,j}\n",
    "$$\n",
    "\n",
    "El siguiente diagrama muestra (a la izquierda) una matriz $A$ de $4\\times 4$, y cómo obtener la submatriz $M_{43}$. A la derecha, vemos los signos $(-1)^{i+j}$ para el caso de una matriz de $4\\times 4$. OBSERVE que la matriz de signos es *simétrica*: tanto la matriz $A$ como $A^T$ tienen la msima matri de signos. \n",
    "\n",
    "<img src= \"imgLA/Fig_264_1.png\", width=800>\n",
    "\n",
    "Los cofactores permiten dar una definición *inductiva* de determinante. La fórmula, para una matriz $n\\times n$ $A=a_{ij}$ es (en este caso, \"desarrollando por la primera fila\"):\n",
    "\n",
    "$$\n",
    "\\mathrm{det\\,} A:= a_{11} C_{11} +\\cdots+ a_{1n}C_{1n}\\qquad\\qquad (3)\n",
    "$$\n",
    "\n",
    "La fórmula (3) reduce el cálculo del determinante de una matriz $n\\times n$ vía cofactores, al cálculo del determinante de matrices $(n-1)\\times (n-1)$. Por ejemplo:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "0 & 0 &1\\\\\n",
    "0 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{pmatrix}\n",
    "\\quad \\Rightarrow\\quad \\mathrm{det\\,} A = 0\\, C_{11}+0\\, C_{12}+ 1\\, C_{13}=(+1)\\,\\mathrm{det\\,} M_{13}= \\mathrm{det\\,} \n",
    "\\begin{pmatrix}\n",
    "0 & 2\\\\\n",
    "4 & 5\n",
    "\\end{pmatrix}\n",
    "= -8\n",
    "$$\n",
    "\n",
    "También podemos calcular $\\mathrm{det\\,} A$ desarrollando por la 3ra. columna:\n",
    "\n",
    "$$\n",
    "\\mathrm{det\\,}A= 1\\, C_{13} - 3\\, C_{23} + 5\\, C_{33}= \\mathrm{det\\,}\n",
    "\\begin{pmatrix}\n",
    "0 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{pmatrix}\n",
    "-3\\, \\mathrm{det\\,}\n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "4 & 5\n",
    "\\end{pmatrix}\n",
    "+ 5\\, \\mathrm{det\\,}\n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}\n",
    "= -8 + 0 + 0=-8\n",
    "$$\n",
    "\n",
    "El libro enumera 3 maneras diferentes de calcular el determinante: por pivotes, por la \"fórmula grande\" y por cofactores. Pivotes es, más o menos, una treta calculatoria. Las otras dos son definiciones generales de determinante. Hemos visto cómo funciona la fórmula inductiva de los cofactores. La \"fórmula grande\" es la definición general: consta de todos los $n!$ productos posibles de elementos de $A$, de manera tal que en cada producta haya exactamente un elemento de cada fila y uno de cada columna. Por ejemolo, para una matriz $3\\times 3$:\n",
    "\n",
    "<img src= \"imgLA/Fig_260_1.png\", width=800>\n",
    "\n",
    "La *fórmula grande* permite demostrar, por ejemplo, que $\\mathrm{det\\,}A = \\mathrm{det\\,}A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"imgLA/Fig_273_1.png\", width=800>\n",
    "\n",
    "#### Regla de Cramer\n",
    "Resuelve sistemas *cuadrados* de ecuaciones $A\\mathbf{x}=\\mathbf{b}$. No es eficiente computacionalmente.\n",
    "\n",
    "<img src= \"imgLA/Fig_273_2.png\", width=800>\n",
    "\n",
    "#### La inversa por cofactores\n",
    "Con la expansión por cofactores podemos calcular directamente la inversa de una matriz (invertible). \n",
    "\n",
    "<img src= \"imgLA/Fig_275_1.png\", width=800>\n",
    "\n",
    "Para demostrar esta fórmula, formemos la matriz de cofactores $C=(C_{ij})$. Con esta notación, la fórmula de la inversa queda \n",
    "\n",
    "$$\n",
    "\\big(\\mathrm{det \\,}A\\big)\\, A^{-1}=C.\n",
    "$$\n",
    "\n",
    "Como la inversa (si existe) es única, basta con probar que $AC$ es la matriz diagonal con $(\\mathrm{det\\,}A)$ en cada entrada de la diagonal.\n",
    "\n",
    "<img src= \"imgLA/Fig_275_2.png\", width=800>\n",
    "\n",
    "Fuera de la diagonal, los téwrminos son $0$:\n",
    "\n",
    "<img src= \"imgLA/Fig_275_3.png\", width=800>\n",
    "\n",
    "#### Interpretación geométrica del determinante.\n",
    "La interpretación geométrica del determinante es *area* (para matrices $2\\times 2$) o *volúmen* (para matrices $3\\times 3$) y, más generalmente, *volúmen $n$-dimensional* para mstrices $n\\times n$, para $n\\geq 2$. OBSERVACIÓN IMPORTANTE: el determinante nos da área, volúmen, $n$-volúmen **orientados**; si queremos considerar el volúmen *sin signo* (o sea, con signo siempre positivo), debemos tomar el valor absoluto $|\\mathrm{det\\,}A  |$.\n",
    "\n",
    "<img src= \"imgLA/Fig_277_1.png\", width=800>\n",
    "\n",
    "Para el caso 3-dimensional, tenemos:\n",
    "\n",
    "<img src= \"imgLA/Fig_278_1.png\", width=800>\n",
    "\n",
    "### DEFINICIÓN de *producto exterior* de vectores de $\\mathbb{R}^3$\n",
    "\n",
    "<img src= \"imgLA/Fig_279_1.png\", width=800>\n",
    "\n",
    "Algunas propiedades del producto exterior:\n",
    "\n",
    "<img src= \"imgLA/Fig_279_2.png\", width=800>\n",
    "\n",
    "<img src= \"imgLA/Fig_280_3.png\", width=800>\n",
    "\n",
    "Como en el caso del producto escalar, en el caso del producto vectorial hay también un otra definición más geométrica:\n",
    "\n",
    "<img src= \"imgLA/Fig_280_1.png\", width=800>\n",
    "\n",
    "\n",
    "Esta otra definición se demuestra fácilmente es equivalente a la ya dada, observando las siguientes identidades:\n",
    "\n",
    "<img src= \"imgLA/Fig_281_1.png\", width=800>\n",
    "\n",
    "Resumiendo, para las longitudes de los dos productos, tenemos:\n",
    "\n",
    "<img src= \"imgLA/Fig_280_2.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Lea los ejercicios marcados con $*$ (y sepa el resultado), y decida si quiere intentar resolverlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{4.1}$ $\\qquad$ \n",
    "4-7, 10-11, 18, 28, 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{4.2}$ $\\qquad$ \n",
    "4, 13, 17, 19, 23, 25, 26, 29, 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{4.3}$ $\\qquad$ \n",
    "4.3A y 4.3B (problemas resueltos!) 1-4, 12-16, 25-7, 29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{4.4}$ $\\qquad$ \n",
    "1-12, 19-24, 34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{5.1}$ $\\qquad$ \n",
    "1, 3-6, 8-10, 18-9, 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{5.2}$ $\\qquad$ \n",
    "2-6, 12-3, 15-6, 33-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ $\\mathbf{5.3}$ $\\qquad$ \n",
    "1, 6-7, 16, 20, 26, 39."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
